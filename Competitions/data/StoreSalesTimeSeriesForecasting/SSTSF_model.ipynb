{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import eli5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the data we have here\n",
    "\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "stores = pd.read_csv(\"./stores.csv\")\n",
    "transactions = pd.read_csv(\"./transactions.csv\")\n",
    "h_days = pd.read_csv(\"./holidays_events.csv\")\n",
    "oil = pd.read_csv(\"./oil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date  store_nbr      family  sales  onpromotion\n",
       "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
       "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
       "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
       "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
       "4   4  2013-01-01          1       BOOKS    0.0            0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, m_df, on = 'date'):\n",
    " df = pd.merge(how = 'left', on = on, left = df, right = m_df)\n",
    "\n",
    " return df\n",
    "\n",
    "train = merge(train, oil, \"date\")\n",
    "train = merge(train, h_days)\n",
    "train = merge(train, stores, \"store_nbr\")\n",
    "train = merge(train, transactions, [\"store_nbr\", \"date\"])\n",
    "\n",
    "test = merge(test, oil, \"date\")\n",
    "test = merge(test, h_days)\n",
    "test = merge(test, stores, \"store_nbr\")\n",
    "test = merge(test, transactions, [\"store_nbr\", \"date\"])\n",
    "\n",
    "train = train.drop(['store_nbr'], axis = 1)\n",
    "test = test.drop(['store_nbr'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'2012-03-02' in h_days['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n",
    "               utc=True)\n",
    "\n",
    "def split_date(df):\n",
    " all_hdays = h_days['date'].unique()\n",
    "\n",
    " df['is_holiday'] = df['date'].apply(lambda x: 1 if x in all_hdays else 0)\n",
    " \n",
    " df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    " df['day'] = df['date'].apply(lambda x: x.day)\n",
    " df['month'] = df['date'].apply(lambda x: x.month)\n",
    " df['year'] = df['date'].apply(lambda x: x.year)\n",
    " df['quarter'] = df['date'].apply(lambda x: x.quarter)\n",
    "\n",
    " df = df.drop(['date'], axis = 1)\n",
    "\n",
    " return df\n",
    "\n",
    "train = split_date(train)\n",
    "test = split_date(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    " le = LabelEncoder()\n",
    " cols_to_encode = [\"family\", \"state\", \"type_holiday\", \"type_store\", \"locale\", \"locale_name\", \"transferred\", \"city\"]\n",
    "\n",
    " for c in cols_to_encode:\n",
    "  df[c] = le.fit_transform(df[c])\n",
    " \n",
    " return df\n",
    "\n",
    "train = encode(train)\n",
    "test = encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"is_holiday\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"type_store\"], axis = 1)\n",
    "test = test.drop([\"type_store\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "family                0\n",
       "sales                 0\n",
       "onpromotion           0\n",
       "dcoilwtico       955152\n",
       "type_holiday          0\n",
       "locale                0\n",
       "locale_name           0\n",
       "description     2551824\n",
       "transferred           0\n",
       "city                  0\n",
       "state                 0\n",
       "cluster               0\n",
       "transactions     249117\n",
       "is_holiday            0\n",
       "day                   0\n",
       "month                 0\n",
       "year                  0\n",
       "quarter               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill the missing data:\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2099196,)\n",
      "(2099196, 16)\n",
      "[102. 102. 102. ...  31.  31.  31.]\n",
      "(21384,)\n",
      "(21384, 15)\n",
      "[48. 48. 48. ... 47. 47. 47.]\n"
     ]
    }
   ],
   "source": [
    "# Let's predict the missing values (for the oil column)\n",
    "\n",
    "def predict_oil(df):\n",
    "\n",
    " first_copy = df.copy()\n",
    " first_copy = first_copy.drop(['description', \"transactions\"], axis = 1)\n",
    "\n",
    " oil_test_data = first_copy[first_copy[\"dcoilwtico\"].isnull()]\n",
    " second_copy = first_copy.copy()\n",
    " second_copy.dropna(inplace=True)\n",
    " \n",
    " oil_y_train = second_copy[\"dcoilwtico\"]\n",
    " oil_x_train = second_copy.drop(['dcoilwtico'], axis = 1)\n",
    " oil_x_test = oil_test_data.drop(['dcoilwtico'], axis = 1)\n",
    "\n",
    " print(oil_y_train.shape)\n",
    " print(oil_x_train.shape)\n",
    " model = LinearRegression()\n",
    " model.fit(oil_x_train, oil_y_train)\n",
    "\n",
    " oil_y_test = model.predict(oil_x_test)\n",
    "\n",
    " oil_y_test = np.round(oil_y_test, decimals = 0)\n",
    "\n",
    " print(oil_y_test)\n",
    "\n",
    " indices = df[df[\"dcoilwtico\"].isnull()].index\n",
    "\n",
    " for fill_index, dataframe_index in enumerate(indices):\n",
    "  df.loc[dataframe_index, \"dcoilwtico\"] = oil_y_test[fill_index]\n",
    "\n",
    " return df\n",
    "\n",
    "train = predict_oil(train)\n",
    "test = predict_oil(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "family                0\n",
       "sales                 0\n",
       "onpromotion           0\n",
       "dcoilwtico            0\n",
       "type_holiday          0\n",
       "locale                0\n",
       "locale_name           0\n",
       "description     2551824\n",
       "transferred           0\n",
       "city                  0\n",
       "state                 0\n",
       "cluster               0\n",
       "transactions     249117\n",
       "is_holiday            0\n",
       "day                   0\n",
       "month                 0\n",
       "year                  0\n",
       "quarter               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "family                0\n",
       "sales                 0\n",
       "onpromotion           0\n",
       "dcoilwtico            0\n",
       "type_holiday          0\n",
       "locale                0\n",
       "locale_name           0\n",
       "description     2551824\n",
       "transferred           0\n",
       "city                  0\n",
       "state                 0\n",
       "cluster               0\n",
       "transactions     249117\n",
       "is_holiday            0\n",
       "day                   0\n",
       "month                 0\n",
       "year                  0\n",
       "quarter               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2805231,)\n",
      "(2805231, 16)\n",
      "[2140. 2142. 2145. ... 1736. 1738. 1740.]\n"
     ]
    }
   ],
   "source": [
    "# Let's predict the missing values (for the oil column)\n",
    "\n",
    "def predict_transactions(df):\n",
    "\n",
    " first_copy = df.copy()\n",
    " first_copy = first_copy.drop(['description', \"dcoilwtico\"], axis=1)\n",
    "\n",
    " oil_test_data = first_copy[first_copy[\"transactions\"].isnull()]\n",
    " second_copy = first_copy.copy()\n",
    " second_copy.dropna(inplace=True)\n",
    "\n",
    " oil_y_train = second_copy[\"transactions\"]\n",
    " oil_x_train = second_copy.drop(['transactions'], axis=1)\n",
    " oil_x_test = oil_test_data.drop(['transactions'], axis=1)\n",
    "\n",
    " print(oil_y_train.shape)\n",
    " print(oil_x_train.shape)\n",
    " model = LinearRegression()\n",
    " model.fit(oil_x_train, oil_y_train)\n",
    "\n",
    " oil_y_test = model.predict(oil_x_test)\n",
    "\n",
    " oil_y_test = np.round(oil_y_test, decimals=0)\n",
    "\n",
    " print(oil_y_test)\n",
    "\n",
    " indices = df[df[\"transactions\"].isnull()].index\n",
    "\n",
    " for fill_index, dataframe_index in enumerate(indices):\n",
    "  df.loc[dataframe_index, \"transactions\"] = oil_y_test[fill_index]\n",
    "\n",
    " return df\n",
    "\n",
    "\n",
    "train = predict_transactions(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['description'], axis = 1)\n",
    "test = test.drop(['description'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "def f_eng(df):\n",
    " oil_mean_price = df['dcoilwtico'].describe()[\"mean\"]\n",
    " df['after_paycheck'] = df['day'].apply(lambda x: 1 if (15 <= x <= 18) or (29 <= x <= 31) or (0 <= x <= 2) else 0)\n",
    "\n",
    " return df\n",
    "\n",
    "train = f_eng(train)\n",
    "test = f_eng(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_49460_row0_col0, #T_49460_row1_col1, #T_49460_row2_col2, #T_49460_row3_col3, #T_49460_row4_col4, #T_49460_row5_col5, #T_49460_row6_col6, #T_49460_row7_col7, #T_49460_row8_col8, #T_49460_row9_col9, #T_49460_row10_col10, #T_49460_row11_col11, #T_49460_row12_col12, #T_49460_row13_col13, #T_49460_row14_col14, #T_49460_row15_col15, #T_49460_row16_col16, #T_49460_row17_col17, #T_49460_row18_col18 {\n",
       "  background-color: #662506;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row0_col1, #T_49460_row1_col3, #T_49460_row4_col1, #T_49460_row5_col1, #T_49460_row6_col1, #T_49460_row6_col14, #T_49460_row7_col1, #T_49460_row8_col1, #T_49460_row9_col1, #T_49460_row10_col1, #T_49460_row11_col1, #T_49460_row12_col1, #T_49460_row13_col1, #T_49460_row14_col1, #T_49460_row15_col1, #T_49460_row16_col1, #T_49460_row17_col1, #T_49460_row18_col1 {\n",
       "  background-color: #fff8c4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col2 {\n",
       "  background-color: #ffefaa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col3 {\n",
       "  background-color: #fed26d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col4, #T_49460_row0_col9, #T_49460_row0_col10, #T_49460_row0_col11, #T_49460_row1_col2, #T_49460_row1_col9, #T_49460_row1_col10, #T_49460_row1_col11, #T_49460_row2_col1, #T_49460_row4_col0, #T_49460_row4_col3, #T_49460_row4_col9, #T_49460_row4_col10, #T_49460_row4_col11, #T_49460_row4_col16, #T_49460_row5_col9, #T_49460_row5_col10, #T_49460_row5_col11, #T_49460_row5_col12, #T_49460_row5_col15, #T_49460_row5_col17, #T_49460_row6_col9, #T_49460_row6_col10, #T_49460_row6_col11, #T_49460_row7_col9, #T_49460_row7_col10, #T_49460_row7_col11, #T_49460_row8_col9, #T_49460_row8_col10, #T_49460_row8_col11, #T_49460_row8_col13, #T_49460_row13_col5, #T_49460_row13_col6, #T_49460_row13_col7, #T_49460_row13_col8, #T_49460_row13_col9, #T_49460_row13_col10, #T_49460_row13_col11, #T_49460_row13_col14, #T_49460_row13_col18, #T_49460_row14_col9, #T_49460_row14_col10, #T_49460_row14_col11, #T_49460_row15_col9, #T_49460_row15_col10, #T_49460_row15_col11, #T_49460_row16_col4, #T_49460_row16_col9, #T_49460_row16_col10, #T_49460_row16_col11, #T_49460_row17_col9, #T_49460_row17_col10, #T_49460_row17_col11, #T_49460_row18_col9, #T_49460_row18_col10, #T_49460_row18_col11 {\n",
       "  background-color: #ffffe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col5, #T_49460_row0_col7, #T_49460_row5_col16, #T_49460_row6_col16, #T_49460_row8_col16, #T_49460_row12_col0, #T_49460_row12_col7 {\n",
       "  background-color: #fea937;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col6, #T_49460_row1_col0, #T_49460_row1_col4, #T_49460_row1_col16, #T_49460_row9_col0, #T_49460_row9_col4, #T_49460_row9_col16, #T_49460_row10_col0, #T_49460_row10_col4, #T_49460_row10_col16, #T_49460_row11_col0, #T_49460_row11_col4, #T_49460_row11_col16, #T_49460_row18_col0, #T_49460_row18_col4 {\n",
       "  background-color: #fea433;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col8, #T_49460_row2_col7, #T_49460_row3_col5, #T_49460_row4_col13, #T_49460_row12_col8, #T_49460_row16_col5 {\n",
       "  background-color: #fea231;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col12, #T_49460_row2_col14 {\n",
       "  background-color: #fffcd7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col13, #T_49460_row18_col5 {\n",
       "  background-color: #fa9125;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row0_col14, #T_49460_row3_col1, #T_49460_row7_col15, #T_49460_row15_col14, #T_49460_row17_col14 {\n",
       "  background-color: #fffbd2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col15, #T_49460_row0_col17 {\n",
       "  background-color: #feeca5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row0_col16, #T_49460_row16_col0 {\n",
       "  background-color: #6b2606;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row0_col18, #T_49460_row4_col18, #T_49460_row16_col18 {\n",
       "  background-color: #ffface;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col5, #T_49460_row2_col6, #T_49460_row3_col2, #T_49460_row6_col4, #T_49460_row9_col5, #T_49460_row10_col5, #T_49460_row11_col5, #T_49460_row16_col8 {\n",
       "  background-color: #fe9e2d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col6, #T_49460_row2_col8, #T_49460_row3_col8, #T_49460_row8_col4, #T_49460_row9_col6, #T_49460_row10_col6, #T_49460_row11_col6 {\n",
       "  background-color: #fe9c2c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col7, #T_49460_row3_col6, #T_49460_row9_col7, #T_49460_row10_col7, #T_49460_row11_col7, #T_49460_row13_col16 {\n",
       "  background-color: #fe9f2e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col8, #T_49460_row1_col13, #T_49460_row7_col4, #T_49460_row9_col8, #T_49460_row9_col13, #T_49460_row10_col8, #T_49460_row10_col13, #T_49460_row11_col8, #T_49460_row11_col13, #T_49460_row13_col0, #T_49460_row14_col5, #T_49460_row14_col7 {\n",
       "  background-color: #fe9a2a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col12, #T_49460_row2_col10 {\n",
       "  background-color: #fffbcf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col14, #T_49460_row3_col14, #T_49460_row4_col14, #T_49460_row9_col14, #T_49460_row10_col14, #T_49460_row11_col14 {\n",
       "  background-color: #fffbd3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col15, #T_49460_row1_col17, #T_49460_row9_col15, #T_49460_row9_col17, #T_49460_row10_col15, #T_49460_row10_col17, #T_49460_row11_col15, #T_49460_row11_col17 {\n",
       "  background-color: #fff6b9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row1_col18, #T_49460_row3_col18, #T_49460_row5_col14, #T_49460_row9_col18, #T_49460_row10_col18, #T_49460_row11_col18 {\n",
       "  background-color: #fffacd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col0, #T_49460_row4_col7, #T_49460_row14_col6, #T_49460_row16_col13, #T_49460_row18_col7 {\n",
       "  background-color: #fc9427;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col3, #T_49460_row2_col16, #T_49460_row3_col13, #T_49460_row4_col6, #T_49460_row9_col10, #T_49460_row10_col9 {\n",
       "  background-color: #fd9627;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col4, #T_49460_row15_col6, #T_49460_row17_col6 {\n",
       "  background-color: #feb340;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col5, #T_49460_row12_col4, #T_49460_row16_col6 {\n",
       "  background-color: #fea030;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col9, #T_49460_row7_col17, #T_49460_row14_col12 {\n",
       "  background-color: #fffcd6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col11, #T_49460_row12_col14, #T_49460_row16_col12 {\n",
       "  background-color: #fffdd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col12 {\n",
       "  background-color: #fedf88;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col13 {\n",
       "  background-color: #fd9728;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col15, #T_49460_row10_col3 {\n",
       "  background-color: #fff3b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col17 {\n",
       "  background-color: #fff3b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row2_col18, #T_49460_row18_col12 {\n",
       "  background-color: #fffacb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col0 {\n",
       "  background-color: #f3801c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row3_col4 {\n",
       "  background-color: #fec24d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col7, #T_49460_row12_col6, #T_49460_row14_col0, #T_49460_row14_col4, #T_49460_row14_col13, #T_49460_row15_col4, #T_49460_row17_col4 {\n",
       "  background-color: #fea332;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col9, #T_49460_row3_col11 {\n",
       "  background-color: #ffffe4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col10, #T_49460_row16_col15 {\n",
       "  background-color: #fffee1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col12, #T_49460_row6_col2, #T_49460_row7_col2, #T_49460_row14_col2 {\n",
       "  background-color: #fff9c7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col15, #T_49460_row3_col17, #T_49460_row13_col3 {\n",
       "  background-color: #fff2b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row3_col16 {\n",
       "  background-color: #f4811d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row4_col2, #T_49460_row6_col12, #T_49460_row8_col15 {\n",
       "  background-color: #fffddc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row4_col5 {\n",
       "  background-color: #fb9326;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row4_col8, #T_49460_row12_col13 {\n",
       "  background-color: #fb9225;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row4_col12, #T_49460_row15_col18, #T_49460_row17_col18 {\n",
       "  background-color: #fffaca;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row4_col15, #T_49460_row4_col17, #T_49460_row14_col3, #T_49460_row14_col15 {\n",
       "  background-color: #fff4b6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row5_col0, #T_49460_row7_col0 {\n",
       "  background-color: #feae3b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row5_col2, #T_49460_row7_col14, #T_49460_row8_col2, #T_49460_row12_col18 {\n",
       "  background-color: #fff9c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row5_col3, #T_49460_row6_col3, #T_49460_row7_col3, #T_49460_row8_col3 {\n",
       "  background-color: #fff7bc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row5_col4, #T_49460_row15_col0, #T_49460_row17_col0 {\n",
       "  background-color: #fe9829;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row5_col6, #T_49460_row5_col7, #T_49460_row6_col5, #T_49460_row7_col5 {\n",
       "  background-color: #7b2b05;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row5_col8, #T_49460_row8_col5 {\n",
       "  background-color: #712806;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row5_col13, #T_49460_row6_col15 {\n",
       "  background-color: #fffddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row5_col18, #T_49460_row8_col18, #T_49460_row9_col2, #T_49460_row9_col3, #T_49460_row11_col3, #T_49460_row18_col3, #T_49460_row18_col15, #T_49460_row18_col17 {\n",
       "  background-color: #fff4b5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row6_col0, #T_49460_row8_col0, #T_49460_row13_col4 {\n",
       "  background-color: #fead3a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row6_col7, #T_49460_row7_col6 {\n",
       "  background-color: #812d05;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row6_col8, #T_49460_row8_col6 {\n",
       "  background-color: #6e2706;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row6_col13, #T_49460_row6_col17, #T_49460_row16_col17 {\n",
       "  background-color: #fffee0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row6_col18, #T_49460_row10_col2, #T_49460_row12_col3, #T_49460_row12_col17, #T_49460_row15_col3, #T_49460_row17_col3 {\n",
       "  background-color: #fff1ae;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row7_col8 {\n",
       "  background-color: #742905;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row7_col12, #T_49460_row8_col12, #T_49460_row8_col17 {\n",
       "  background-color: #fffedf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row7_col13 {\n",
       "  background-color: #fffddb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row7_col16 {\n",
       "  background-color: #feab39;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row7_col18 {\n",
       "  background-color: #fff6ba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row8_col7 {\n",
       "  background-color: #762a05;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row8_col14, #T_49460_row13_col12 {\n",
       "  background-color: #fff8c1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row9_col11, #T_49460_row11_col9 {\n",
       "  background-color: #fff9c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row9_col12 {\n",
       "  background-color: #fed573;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row10_col11, #T_49460_row11_col10 {\n",
       "  background-color: #fff0ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row10_col12 {\n",
       "  background-color: #feb744;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row11_col2, #T_49460_row14_col17, #T_49460_row14_col18 {\n",
       "  background-color: #fff5b8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row11_col12, #T_49460_row12_col9 {\n",
       "  background-color: #fee18c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row12_col2 {\n",
       "  background-color: #fed87a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row12_col5, #T_49460_row12_col16 {\n",
       "  background-color: #feaa38;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row12_col10 {\n",
       "  background-color: #fec652;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row12_col11 {\n",
       "  background-color: #feeaa1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row12_col15, #T_49460_row16_col2 {\n",
       "  background-color: #ffefac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row13_col2 {\n",
       "  background-color: #fff8c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row13_col15 {\n",
       "  background-color: #fee493;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row13_col17 {\n",
       "  background-color: #fee392;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row14_col8 {\n",
       "  background-color: #fa8f24;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row14_col16, #T_49460_row18_col13 {\n",
       "  background-color: #fea736;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row15_col2, #T_49460_row17_col2 {\n",
       "  background-color: #fff7be;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row15_col5, #T_49460_row17_col5 {\n",
       "  background-color: #feba46;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row15_col7, #T_49460_row15_col8 {\n",
       "  background-color: #feaf3d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row15_col12, #T_49460_row18_col2 {\n",
       "  background-color: #fff8c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row15_col13 {\n",
       "  background-color: #f5851f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row15_col16 {\n",
       "  background-color: #febf4b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row15_col17, #T_49460_row17_col15 {\n",
       "  background-color: #702806;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row16_col3 {\n",
       "  background-color: #fed36f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row16_col7, #T_49460_row18_col16 {\n",
       "  background-color: #fea634;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row16_col14 {\n",
       "  background-color: #fffcd8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row17_col7, #T_49460_row17_col8 {\n",
       "  background-color: #feb13e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row17_col12 {\n",
       "  background-color: #fff9c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row17_col13 {\n",
       "  background-color: #f5841e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row17_col16 {\n",
       "  background-color: #febe4a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49460_row18_col6 {\n",
       "  background-color: #f88b22;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row18_col8 {\n",
       "  background-color: #f98d23;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49460_row18_col14 {\n",
       "  background-color: #fff7bd;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_49460\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_49460_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
       "      <th id=\"T_49460_level0_col1\" class=\"col_heading level0 col1\" >family</th>\n",
       "      <th id=\"T_49460_level0_col2\" class=\"col_heading level0 col2\" >sales</th>\n",
       "      <th id=\"T_49460_level0_col3\" class=\"col_heading level0 col3\" >onpromotion</th>\n",
       "      <th id=\"T_49460_level0_col4\" class=\"col_heading level0 col4\" >dcoilwtico</th>\n",
       "      <th id=\"T_49460_level0_col5\" class=\"col_heading level0 col5\" >type_holiday</th>\n",
       "      <th id=\"T_49460_level0_col6\" class=\"col_heading level0 col6\" >locale</th>\n",
       "      <th id=\"T_49460_level0_col7\" class=\"col_heading level0 col7\" >locale_name</th>\n",
       "      <th id=\"T_49460_level0_col8\" class=\"col_heading level0 col8\" >transferred</th>\n",
       "      <th id=\"T_49460_level0_col9\" class=\"col_heading level0 col9\" >city</th>\n",
       "      <th id=\"T_49460_level0_col10\" class=\"col_heading level0 col10\" >state</th>\n",
       "      <th id=\"T_49460_level0_col11\" class=\"col_heading level0 col11\" >cluster</th>\n",
       "      <th id=\"T_49460_level0_col12\" class=\"col_heading level0 col12\" >transactions</th>\n",
       "      <th id=\"T_49460_level0_col13\" class=\"col_heading level0 col13\" >is_holiday</th>\n",
       "      <th id=\"T_49460_level0_col14\" class=\"col_heading level0 col14\" >day</th>\n",
       "      <th id=\"T_49460_level0_col15\" class=\"col_heading level0 col15\" >month</th>\n",
       "      <th id=\"T_49460_level0_col16\" class=\"col_heading level0 col16\" >year</th>\n",
       "      <th id=\"T_49460_level0_col17\" class=\"col_heading level0 col17\" >quarter</th>\n",
       "      <th id=\"T_49460_level0_col18\" class=\"col_heading level0 col18\" >after_paycheck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row0\" class=\"row_heading level0 row0\" >id</th>\n",
       "      <td id=\"T_49460_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_49460_row0_col1\" class=\"data row0 col1\" >0.000011</td>\n",
       "      <td id=\"T_49460_row0_col2\" class=\"data row0 col2\" >0.086102</td>\n",
       "      <td id=\"T_49460_row0_col3\" class=\"data row0 col3\" >0.206032</td>\n",
       "      <td id=\"T_49460_row0_col4\" class=\"data row0 col4\" >-0.880647</td>\n",
       "      <td id=\"T_49460_row0_col5\" class=\"data row0 col5\" >-0.056797</td>\n",
       "      <td id=\"T_49460_row0_col6\" class=\"data row0 col6\" >-0.049972</td>\n",
       "      <td id=\"T_49460_row0_col7\" class=\"data row0 col7\" >-0.053151</td>\n",
       "      <td id=\"T_49460_row0_col8\" class=\"data row0 col8\" >-0.047792</td>\n",
       "      <td id=\"T_49460_row0_col9\" class=\"data row0 col9\" >0.000028</td>\n",
       "      <td id=\"T_49460_row0_col10\" class=\"data row0 col10\" >0.000078</td>\n",
       "      <td id=\"T_49460_row0_col11\" class=\"data row0 col11\" >-0.000044</td>\n",
       "      <td id=\"T_49460_row0_col12\" class=\"data row0 col12\" >-0.023349</td>\n",
       "      <td id=\"T_49460_row0_col13\" class=\"data row0 col13\" >0.052877</td>\n",
       "      <td id=\"T_49460_row0_col14\" class=\"data row0 col14\" >0.004563</td>\n",
       "      <td id=\"T_49460_row0_col15\" class=\"data row0 col15\" >0.066683</td>\n",
       "      <td id=\"T_49460_row0_col16\" class=\"data row0 col16\" >0.977741</td>\n",
       "      <td id=\"T_49460_row0_col17\" class=\"data row0 col17\" >0.065949</td>\n",
       "      <td id=\"T_49460_row0_col18\" class=\"data row0 col18\" >-0.001084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row1\" class=\"row_heading level0 row1\" >family</th>\n",
       "      <td id=\"T_49460_row1_col0\" class=\"data row1 col0\" >0.000011</td>\n",
       "      <td id=\"T_49460_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_49460_row1_col2\" class=\"data row1 col2\" >-0.113986</td>\n",
       "      <td id=\"T_49460_row1_col3\" class=\"data row1 col3\" >-0.047216</td>\n",
       "      <td id=\"T_49460_row1_col4\" class=\"data row1 col4\" >-0.000237</td>\n",
       "      <td id=\"T_49460_row1_col5\" class=\"data row1 col5\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row1_col6\" class=\"data row1 col6\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row1_col7\" class=\"data row1 col7\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row1_col8\" class=\"data row1 col8\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row1_col10\" class=\"data row1 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row1_col11\" class=\"data row1 col11\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row1_col12\" class=\"data row1 col12\" >0.001815</td>\n",
       "      <td id=\"T_49460_row1_col13\" class=\"data row1 col13\" >0.000000</td>\n",
       "      <td id=\"T_49460_row1_col14\" class=\"data row1 col14\" >0.000000</td>\n",
       "      <td id=\"T_49460_row1_col15\" class=\"data row1 col15\" >0.000000</td>\n",
       "      <td id=\"T_49460_row1_col16\" class=\"data row1 col16\" >0.000000</td>\n",
       "      <td id=\"T_49460_row1_col17\" class=\"data row1 col17\" >0.000000</td>\n",
       "      <td id=\"T_49460_row1_col18\" class=\"data row1 col18\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row2\" class=\"row_heading level0 row2\" >sales</th>\n",
       "      <td id=\"T_49460_row2_col0\" class=\"data row2 col0\" >0.086102</td>\n",
       "      <td id=\"T_49460_row2_col1\" class=\"data row2 col1\" >-0.113986</td>\n",
       "      <td id=\"T_49460_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_49460_row2_col3\" class=\"data row2 col3\" >0.428241</td>\n",
       "      <td id=\"T_49460_row2_col4\" class=\"data row2 col4\" >-0.083383</td>\n",
       "      <td id=\"T_49460_row2_col5\" class=\"data row2 col5\" >-0.017870</td>\n",
       "      <td id=\"T_49460_row2_col6\" class=\"data row2 col6\" >-0.011640</td>\n",
       "      <td id=\"T_49460_row2_col7\" class=\"data row2 col7\" >-0.013896</td>\n",
       "      <td id=\"T_49460_row2_col8\" class=\"data row2 col8\" >-0.014430</td>\n",
       "      <td id=\"T_49460_row2_col9\" class=\"data row2 col9\" >0.049510</td>\n",
       "      <td id=\"T_49460_row2_col10\" class=\"data row2 col10\" >0.068988</td>\n",
       "      <td id=\"T_49460_row2_col11\" class=\"data row2 col11\" >0.038537</td>\n",
       "      <td id=\"T_49460_row2_col12\" class=\"data row2 col12\" >0.213500</td>\n",
       "      <td id=\"T_49460_row2_col13\" class=\"data row2 col13\" >0.013964</td>\n",
       "      <td id=\"T_49460_row2_col14\" class=\"data row2 col14\" >-0.011972</td>\n",
       "      <td id=\"T_49460_row2_col15\" class=\"data row2 col15\" >0.020250</td>\n",
       "      <td id=\"T_49460_row2_col16\" class=\"data row2 col16\" >0.081320</td>\n",
       "      <td id=\"T_49460_row2_col17\" class=\"data row2 col17\" >0.018752</td>\n",
       "      <td id=\"T_49460_row2_col18\" class=\"data row2 col18\" >0.006860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row3\" class=\"row_heading level0 row3\" >onpromotion</th>\n",
       "      <td id=\"T_49460_row3_col0\" class=\"data row3 col0\" >0.206032</td>\n",
       "      <td id=\"T_49460_row3_col1\" class=\"data row3 col1\" >-0.047216</td>\n",
       "      <td id=\"T_49460_row3_col2\" class=\"data row3 col2\" >0.428241</td>\n",
       "      <td id=\"T_49460_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_49460_row3_col4\" class=\"data row3 col4\" >-0.167526</td>\n",
       "      <td id=\"T_49460_row3_col5\" class=\"data row3 col5\" >-0.020360</td>\n",
       "      <td id=\"T_49460_row3_col6\" class=\"data row3 col6\" >-0.017057</td>\n",
       "      <td id=\"T_49460_row3_col7\" class=\"data row3 col7\" >-0.019368</td>\n",
       "      <td id=\"T_49460_row3_col8\" class=\"data row3 col8\" >-0.017828</td>\n",
       "      <td id=\"T_49460_row3_col9\" class=\"data row3 col9\" >0.004390</td>\n",
       "      <td id=\"T_49460_row3_col10\" class=\"data row3 col10\" >0.013109</td>\n",
       "      <td id=\"T_49460_row3_col11\" class=\"data row3 col11\" >0.005702</td>\n",
       "      <td id=\"T_49460_row3_col12\" class=\"data row3 col12\" >0.026224</td>\n",
       "      <td id=\"T_49460_row3_col13\" class=\"data row3 col13\" >0.018989</td>\n",
       "      <td id=\"T_49460_row3_col14\" class=\"data row3 col14\" >0.001107</td>\n",
       "      <td id=\"T_49460_row3_col15\" class=\"data row3 col15\" >0.025600</td>\n",
       "      <td id=\"T_49460_row3_col16\" class=\"data row3 col16\" >0.198913</td>\n",
       "      <td id=\"T_49460_row3_col17\" class=\"data row3 col17\" >0.024860</td>\n",
       "      <td id=\"T_49460_row3_col18\" class=\"data row3 col18\" >0.002311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row4\" class=\"row_heading level0 row4\" >dcoilwtico</th>\n",
       "      <td id=\"T_49460_row4_col0\" class=\"data row4 col0\" >-0.880647</td>\n",
       "      <td id=\"T_49460_row4_col1\" class=\"data row4 col1\" >-0.000237</td>\n",
       "      <td id=\"T_49460_row4_col2\" class=\"data row4 col2\" >-0.083383</td>\n",
       "      <td id=\"T_49460_row4_col3\" class=\"data row4 col3\" >-0.167526</td>\n",
       "      <td id=\"T_49460_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_49460_row4_col5\" class=\"data row4 col5\" >0.060471</td>\n",
       "      <td id=\"T_49460_row4_col6\" class=\"data row4 col6\" >0.036532</td>\n",
       "      <td id=\"T_49460_row4_col7\" class=\"data row4 col7\" >0.057774</td>\n",
       "      <td id=\"T_49460_row4_col8\" class=\"data row4 col8\" >0.044715</td>\n",
       "      <td id=\"T_49460_row4_col9\" class=\"data row4 col9\" >-0.000190</td>\n",
       "      <td id=\"T_49460_row4_col10\" class=\"data row4 col10\" >-0.000259</td>\n",
       "      <td id=\"T_49460_row4_col11\" class=\"data row4 col11\" >-0.000141</td>\n",
       "      <td id=\"T_49460_row4_col12\" class=\"data row4 col12\" >0.016473</td>\n",
       "      <td id=\"T_49460_row4_col13\" class=\"data row4 col13\" >-0.047209</td>\n",
       "      <td id=\"T_49460_row4_col14\" class=\"data row4 col14\" >0.002754</td>\n",
       "      <td id=\"T_49460_row4_col15\" class=\"data row4 col15\" >0.005756</td>\n",
       "      <td id=\"T_49460_row4_col16\" class=\"data row4 col16\" >-0.874675</td>\n",
       "      <td id=\"T_49460_row4_col17\" class=\"data row4 col17\" >0.007028</td>\n",
       "      <td id=\"T_49460_row4_col18\" class=\"data row4 col18\" >-0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row5\" class=\"row_heading level0 row5\" >type_holiday</th>\n",
       "      <td id=\"T_49460_row5_col0\" class=\"data row5 col0\" >-0.056797</td>\n",
       "      <td id=\"T_49460_row5_col1\" class=\"data row5 col1\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row5_col2\" class=\"data row5 col2\" >-0.017870</td>\n",
       "      <td id=\"T_49460_row5_col3\" class=\"data row5 col3\" >-0.020360</td>\n",
       "      <td id=\"T_49460_row5_col4\" class=\"data row5 col4\" >0.060471</td>\n",
       "      <td id=\"T_49460_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_49460_row5_col6\" class=\"data row5 col6\" >0.897414</td>\n",
       "      <td id=\"T_49460_row5_col7\" class=\"data row5 col7\" >0.900716</td>\n",
       "      <td id=\"T_49460_row5_col8\" class=\"data row5 col8\" >0.944148</td>\n",
       "      <td id=\"T_49460_row5_col9\" class=\"data row5 col9\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row5_col10\" class=\"data row5 col10\" >0.000000</td>\n",
       "      <td id=\"T_49460_row5_col11\" class=\"data row5 col11\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row5_col12\" class=\"data row5 col12\" >-0.071515</td>\n",
       "      <td id=\"T_49460_row5_col13\" class=\"data row5 col13\" >-0.945771</td>\n",
       "      <td id=\"T_49460_row5_col14\" class=\"data row5 col14\" >0.019774</td>\n",
       "      <td id=\"T_49460_row5_col15\" class=\"data row5 col15\" >-0.158295</td>\n",
       "      <td id=\"T_49460_row5_col16\" class=\"data row5 col16\" >-0.023633</td>\n",
       "      <td id=\"T_49460_row5_col17\" class=\"data row5 col17\" >-0.156238</td>\n",
       "      <td id=\"T_49460_row5_col18\" class=\"data row5 col18\" >0.078270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row6\" class=\"row_heading level0 row6\" >locale</th>\n",
       "      <td id=\"T_49460_row6_col0\" class=\"data row6 col0\" >-0.049972</td>\n",
       "      <td id=\"T_49460_row6_col1\" class=\"data row6 col1\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row6_col2\" class=\"data row6 col2\" >-0.011640</td>\n",
       "      <td id=\"T_49460_row6_col3\" class=\"data row6 col3\" >-0.017057</td>\n",
       "      <td id=\"T_49460_row6_col4\" class=\"data row6 col4\" >0.036532</td>\n",
       "      <td id=\"T_49460_row6_col5\" class=\"data row6 col5\" >0.897414</td>\n",
       "      <td id=\"T_49460_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_49460_row6_col7\" class=\"data row6 col7\" >0.865778</td>\n",
       "      <td id=\"T_49460_row6_col8\" class=\"data row6 col8\" >0.960054</td>\n",
       "      <td id=\"T_49460_row6_col9\" class=\"data row6 col9\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row6_col10\" class=\"data row6 col10\" >0.000000</td>\n",
       "      <td id=\"T_49460_row6_col11\" class=\"data row6 col11\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row6_col12\" class=\"data row6 col12\" >-0.038598</td>\n",
       "      <td id=\"T_49460_row6_col13\" class=\"data row6 col13\" >-0.963601</td>\n",
       "      <td id=\"T_49460_row6_col14\" class=\"data row6 col14\" >0.048625</td>\n",
       "      <td id=\"T_49460_row6_col15\" class=\"data row6 col15\" >-0.128111</td>\n",
       "      <td id=\"T_49460_row6_col16\" class=\"data row6 col16\" >-0.023697</td>\n",
       "      <td id=\"T_49460_row6_col17\" class=\"data row6 col17\" >-0.134650</td>\n",
       "      <td id=\"T_49460_row6_col18\" class=\"data row6 col18\" >0.097988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row7\" class=\"row_heading level0 row7\" >locale_name</th>\n",
       "      <td id=\"T_49460_row7_col0\" class=\"data row7 col0\" >-0.053151</td>\n",
       "      <td id=\"T_49460_row7_col1\" class=\"data row7 col1\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row7_col2\" class=\"data row7 col2\" >-0.013896</td>\n",
       "      <td id=\"T_49460_row7_col3\" class=\"data row7 col3\" >-0.019368</td>\n",
       "      <td id=\"T_49460_row7_col4\" class=\"data row7 col4\" >0.057774</td>\n",
       "      <td id=\"T_49460_row7_col5\" class=\"data row7 col5\" >0.900716</td>\n",
       "      <td id=\"T_49460_row7_col6\" class=\"data row7 col6\" >0.865778</td>\n",
       "      <td id=\"T_49460_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_49460_row7_col8\" class=\"data row7 col8\" >0.922572</td>\n",
       "      <td id=\"T_49460_row7_col9\" class=\"data row7 col9\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row7_col10\" class=\"data row7 col10\" >0.000000</td>\n",
       "      <td id=\"T_49460_row7_col11\" class=\"data row7 col11\" >0.000000</td>\n",
       "      <td id=\"T_49460_row7_col12\" class=\"data row7 col12\" >-0.050014</td>\n",
       "      <td id=\"T_49460_row7_col13\" class=\"data row7 col13\" >-0.930441</td>\n",
       "      <td id=\"T_49460_row7_col14\" class=\"data row7 col14\" >0.034632</td>\n",
       "      <td id=\"T_49460_row7_col15\" class=\"data row7 col15\" >-0.088518</td>\n",
       "      <td id=\"T_49460_row7_col16\" class=\"data row7 col16\" >-0.034878</td>\n",
       "      <td id=\"T_49460_row7_col17\" class=\"data row7 col17\" >-0.098370</td>\n",
       "      <td id=\"T_49460_row7_col18\" class=\"data row7 col18\" >0.061258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row8\" class=\"row_heading level0 row8\" >transferred</th>\n",
       "      <td id=\"T_49460_row8_col0\" class=\"data row8 col0\" >-0.047792</td>\n",
       "      <td id=\"T_49460_row8_col1\" class=\"data row8 col1\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row8_col2\" class=\"data row8 col2\" >-0.014430</td>\n",
       "      <td id=\"T_49460_row8_col3\" class=\"data row8 col3\" >-0.017828</td>\n",
       "      <td id=\"T_49460_row8_col4\" class=\"data row8 col4\" >0.044715</td>\n",
       "      <td id=\"T_49460_row8_col5\" class=\"data row8 col5\" >0.944148</td>\n",
       "      <td id=\"T_49460_row8_col6\" class=\"data row8 col6\" >0.960054</td>\n",
       "      <td id=\"T_49460_row8_col7\" class=\"data row8 col7\" >0.922572</td>\n",
       "      <td id=\"T_49460_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_49460_row8_col9\" class=\"data row8 col9\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row8_col10\" class=\"data row8 col10\" >0.000000</td>\n",
       "      <td id=\"T_49460_row8_col11\" class=\"data row8 col11\" >0.000000</td>\n",
       "      <td id=\"T_49460_row8_col12\" class=\"data row8 col12\" >-0.048957</td>\n",
       "      <td id=\"T_49460_row8_col13\" class=\"data row8 col13\" >-0.995260</td>\n",
       "      <td id=\"T_49460_row8_col14\" class=\"data row8 col14\" >0.058192</td>\n",
       "      <td id=\"T_49460_row8_col15\" class=\"data row8 col15\" >-0.126011</td>\n",
       "      <td id=\"T_49460_row8_col16\" class=\"data row8 col16\" >-0.022134</td>\n",
       "      <td id=\"T_49460_row8_col17\" class=\"data row8 col17\" >-0.131039</td>\n",
       "      <td id=\"T_49460_row8_col18\" class=\"data row8 col18\" >0.079031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row9\" class=\"row_heading level0 row9\" >city</th>\n",
       "      <td id=\"T_49460_row9_col0\" class=\"data row9 col0\" >0.000028</td>\n",
       "      <td id=\"T_49460_row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row9_col2\" class=\"data row9 col2\" >0.049510</td>\n",
       "      <td id=\"T_49460_row9_col3\" class=\"data row9 col3\" >0.004390</td>\n",
       "      <td id=\"T_49460_row9_col4\" class=\"data row9 col4\" >-0.000190</td>\n",
       "      <td id=\"T_49460_row9_col5\" class=\"data row9 col5\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row9_col6\" class=\"data row9 col6\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row9_col7\" class=\"data row9 col7\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row9_col8\" class=\"data row9 col8\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_49460_row9_col10\" class=\"data row9 col10\" >0.511076</td>\n",
       "      <td id=\"T_49460_row9_col11\" class=\"data row9 col11\" >0.093806</td>\n",
       "      <td id=\"T_49460_row9_col12\" class=\"data row9 col12\" >0.258081</td>\n",
       "      <td id=\"T_49460_row9_col13\" class=\"data row9 col13\" >0.000000</td>\n",
       "      <td id=\"T_49460_row9_col14\" class=\"data row9 col14\" >0.000000</td>\n",
       "      <td id=\"T_49460_row9_col15\" class=\"data row9 col15\" >0.000000</td>\n",
       "      <td id=\"T_49460_row9_col16\" class=\"data row9 col16\" >0.000000</td>\n",
       "      <td id=\"T_49460_row9_col17\" class=\"data row9 col17\" >0.000000</td>\n",
       "      <td id=\"T_49460_row9_col18\" class=\"data row9 col18\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row10\" class=\"row_heading level0 row10\" >state</th>\n",
       "      <td id=\"T_49460_row10_col0\" class=\"data row10 col0\" >0.000078</td>\n",
       "      <td id=\"T_49460_row10_col1\" class=\"data row10 col1\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row10_col2\" class=\"data row10 col2\" >0.068988</td>\n",
       "      <td id=\"T_49460_row10_col3\" class=\"data row10 col3\" >0.013109</td>\n",
       "      <td id=\"T_49460_row10_col4\" class=\"data row10 col4\" >-0.000259</td>\n",
       "      <td id=\"T_49460_row10_col5\" class=\"data row10 col5\" >0.000000</td>\n",
       "      <td id=\"T_49460_row10_col6\" class=\"data row10 col6\" >0.000000</td>\n",
       "      <td id=\"T_49460_row10_col7\" class=\"data row10 col7\" >0.000000</td>\n",
       "      <td id=\"T_49460_row10_col8\" class=\"data row10 col8\" >0.000000</td>\n",
       "      <td id=\"T_49460_row10_col9\" class=\"data row10 col9\" >0.511076</td>\n",
       "      <td id=\"T_49460_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "      <td id=\"T_49460_row10_col11\" class=\"data row10 col11\" >0.169536</td>\n",
       "      <td id=\"T_49460_row10_col12\" class=\"data row10 col12\" >0.369124</td>\n",
       "      <td id=\"T_49460_row10_col13\" class=\"data row10 col13\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row10_col14\" class=\"data row10 col14\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row10_col15\" class=\"data row10 col15\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row10_col16\" class=\"data row10 col16\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row10_col17\" class=\"data row10 col17\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row10_col18\" class=\"data row10 col18\" >-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row11\" class=\"row_heading level0 row11\" >cluster</th>\n",
       "      <td id=\"T_49460_row11_col0\" class=\"data row11 col0\" >-0.000044</td>\n",
       "      <td id=\"T_49460_row11_col1\" class=\"data row11 col1\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row11_col2\" class=\"data row11 col2\" >0.038537</td>\n",
       "      <td id=\"T_49460_row11_col3\" class=\"data row11 col3\" >0.005702</td>\n",
       "      <td id=\"T_49460_row11_col4\" class=\"data row11 col4\" >-0.000141</td>\n",
       "      <td id=\"T_49460_row11_col5\" class=\"data row11 col5\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row11_col6\" class=\"data row11 col6\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row11_col7\" class=\"data row11 col7\" >0.000000</td>\n",
       "      <td id=\"T_49460_row11_col8\" class=\"data row11 col8\" >0.000000</td>\n",
       "      <td id=\"T_49460_row11_col9\" class=\"data row11 col9\" >0.093806</td>\n",
       "      <td id=\"T_49460_row11_col10\" class=\"data row11 col10\" >0.169536</td>\n",
       "      <td id=\"T_49460_row11_col11\" class=\"data row11 col11\" >1.000000</td>\n",
       "      <td id=\"T_49460_row11_col12\" class=\"data row11 col12\" >0.205903</td>\n",
       "      <td id=\"T_49460_row11_col13\" class=\"data row11 col13\" >0.000000</td>\n",
       "      <td id=\"T_49460_row11_col14\" class=\"data row11 col14\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row11_col15\" class=\"data row11 col15\" >0.000000</td>\n",
       "      <td id=\"T_49460_row11_col16\" class=\"data row11 col16\" >0.000000</td>\n",
       "      <td id=\"T_49460_row11_col17\" class=\"data row11 col17\" >0.000000</td>\n",
       "      <td id=\"T_49460_row11_col18\" class=\"data row11 col18\" >-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row12\" class=\"row_heading level0 row12\" >transactions</th>\n",
       "      <td id=\"T_49460_row12_col0\" class=\"data row12 col0\" >-0.023349</td>\n",
       "      <td id=\"T_49460_row12_col1\" class=\"data row12 col1\" >0.001815</td>\n",
       "      <td id=\"T_49460_row12_col2\" class=\"data row12 col2\" >0.213500</td>\n",
       "      <td id=\"T_49460_row12_col3\" class=\"data row12 col3\" >0.026224</td>\n",
       "      <td id=\"T_49460_row12_col4\" class=\"data row12 col4\" >0.016473</td>\n",
       "      <td id=\"T_49460_row12_col5\" class=\"data row12 col5\" >-0.071515</td>\n",
       "      <td id=\"T_49460_row12_col6\" class=\"data row12 col6\" >-0.038598</td>\n",
       "      <td id=\"T_49460_row12_col7\" class=\"data row12 col7\" >-0.050014</td>\n",
       "      <td id=\"T_49460_row12_col8\" class=\"data row12 col8\" >-0.048957</td>\n",
       "      <td id=\"T_49460_row12_col9\" class=\"data row12 col9\" >0.258081</td>\n",
       "      <td id=\"T_49460_row12_col10\" class=\"data row12 col10\" >0.369124</td>\n",
       "      <td id=\"T_49460_row12_col11\" class=\"data row12 col11\" >0.205903</td>\n",
       "      <td id=\"T_49460_row12_col12\" class=\"data row12 col12\" >1.000000</td>\n",
       "      <td id=\"T_49460_row12_col13\" class=\"data row12 col13\" >0.047151</td>\n",
       "      <td id=\"T_49460_row12_col14\" class=\"data row12 col14\" >-0.020020</td>\n",
       "      <td id=\"T_49460_row12_col15\" class=\"data row12 col15\" >0.043271</td>\n",
       "      <td id=\"T_49460_row12_col16\" class=\"data row12 col16\" >-0.032061</td>\n",
       "      <td id=\"T_49460_row12_col17\" class=\"data row12 col17\" >0.034408</td>\n",
       "      <td id=\"T_49460_row12_col18\" class=\"data row12 col18\" >0.014570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row13\" class=\"row_heading level0 row13\" >is_holiday</th>\n",
       "      <td id=\"T_49460_row13_col0\" class=\"data row13 col0\" >0.052877</td>\n",
       "      <td id=\"T_49460_row13_col1\" class=\"data row13 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row13_col2\" class=\"data row13 col2\" >0.013964</td>\n",
       "      <td id=\"T_49460_row13_col3\" class=\"data row13 col3\" >0.018989</td>\n",
       "      <td id=\"T_49460_row13_col4\" class=\"data row13 col4\" >-0.047209</td>\n",
       "      <td id=\"T_49460_row13_col5\" class=\"data row13 col5\" >-0.945771</td>\n",
       "      <td id=\"T_49460_row13_col6\" class=\"data row13 col6\" >-0.963601</td>\n",
       "      <td id=\"T_49460_row13_col7\" class=\"data row13 col7\" >-0.930441</td>\n",
       "      <td id=\"T_49460_row13_col8\" class=\"data row13 col8\" >-0.995260</td>\n",
       "      <td id=\"T_49460_row13_col9\" class=\"data row13 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row13_col10\" class=\"data row13 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row13_col11\" class=\"data row13 col11\" >0.000000</td>\n",
       "      <td id=\"T_49460_row13_col12\" class=\"data row13 col12\" >0.047151</td>\n",
       "      <td id=\"T_49460_row13_col13\" class=\"data row13 col13\" >1.000000</td>\n",
       "      <td id=\"T_49460_row13_col14\" class=\"data row13 col14\" >-0.059048</td>\n",
       "      <td id=\"T_49460_row13_col15\" class=\"data row13 col15\" >0.125056</td>\n",
       "      <td id=\"T_49460_row13_col16\" class=\"data row13 col16\" >0.027392</td>\n",
       "      <td id=\"T_49460_row13_col17\" class=\"data row13 col17\" >0.131197</td>\n",
       "      <td id=\"T_49460_row13_col18\" class=\"data row13 col18\" >-0.080707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row14\" class=\"row_heading level0 row14\" >day</th>\n",
       "      <td id=\"T_49460_row14_col0\" class=\"data row14 col0\" >0.004563</td>\n",
       "      <td id=\"T_49460_row14_col1\" class=\"data row14 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row14_col2\" class=\"data row14 col2\" >-0.011972</td>\n",
       "      <td id=\"T_49460_row14_col3\" class=\"data row14 col3\" >0.001107</td>\n",
       "      <td id=\"T_49460_row14_col4\" class=\"data row14 col4\" >0.002754</td>\n",
       "      <td id=\"T_49460_row14_col5\" class=\"data row14 col5\" >0.019774</td>\n",
       "      <td id=\"T_49460_row14_col6\" class=\"data row14 col6\" >0.048625</td>\n",
       "      <td id=\"T_49460_row14_col7\" class=\"data row14 col7\" >0.034632</td>\n",
       "      <td id=\"T_49460_row14_col8\" class=\"data row14 col8\" >0.058192</td>\n",
       "      <td id=\"T_49460_row14_col9\" class=\"data row14 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row14_col10\" class=\"data row14 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row14_col11\" class=\"data row14 col11\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row14_col12\" class=\"data row14 col12\" >-0.020020</td>\n",
       "      <td id=\"T_49460_row14_col13\" class=\"data row14 col13\" >-0.059048</td>\n",
       "      <td id=\"T_49460_row14_col14\" class=\"data row14 col14\" >1.000000</td>\n",
       "      <td id=\"T_49460_row14_col15\" class=\"data row14 col15\" >0.005777</td>\n",
       "      <td id=\"T_49460_row14_col16\" class=\"data row14 col16\" >-0.014563</td>\n",
       "      <td id=\"T_49460_row14_col17\" class=\"data row14 col17\" >0.004309</td>\n",
       "      <td id=\"T_49460_row14_col18\" class=\"data row14 col18\" >0.070454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row15\" class=\"row_heading level0 row15\" >month</th>\n",
       "      <td id=\"T_49460_row15_col0\" class=\"data row15 col0\" >0.066683</td>\n",
       "      <td id=\"T_49460_row15_col1\" class=\"data row15 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row15_col2\" class=\"data row15 col2\" >0.020250</td>\n",
       "      <td id=\"T_49460_row15_col3\" class=\"data row15 col3\" >0.025600</td>\n",
       "      <td id=\"T_49460_row15_col4\" class=\"data row15 col4\" >0.005756</td>\n",
       "      <td id=\"T_49460_row15_col5\" class=\"data row15 col5\" >-0.158295</td>\n",
       "      <td id=\"T_49460_row15_col6\" class=\"data row15 col6\" >-0.128111</td>\n",
       "      <td id=\"T_49460_row15_col7\" class=\"data row15 col7\" >-0.088518</td>\n",
       "      <td id=\"T_49460_row15_col8\" class=\"data row15 col8\" >-0.126011</td>\n",
       "      <td id=\"T_49460_row15_col9\" class=\"data row15 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row15_col10\" class=\"data row15 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row15_col11\" class=\"data row15 col11\" >0.000000</td>\n",
       "      <td id=\"T_49460_row15_col12\" class=\"data row15 col12\" >0.043271</td>\n",
       "      <td id=\"T_49460_row15_col13\" class=\"data row15 col13\" >0.125056</td>\n",
       "      <td id=\"T_49460_row15_col14\" class=\"data row15 col14\" >0.005777</td>\n",
       "      <td id=\"T_49460_row15_col15\" class=\"data row15 col15\" >1.000000</td>\n",
       "      <td id=\"T_49460_row15_col16\" class=\"data row15 col16\" >-0.143380</td>\n",
       "      <td id=\"T_49460_row15_col17\" class=\"data row15 col17\" >0.970032</td>\n",
       "      <td id=\"T_49460_row15_col18\" class=\"data row15 col18\" >0.009678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row16\" class=\"row_heading level0 row16\" >year</th>\n",
       "      <td id=\"T_49460_row16_col0\" class=\"data row16 col0\" >0.977741</td>\n",
       "      <td id=\"T_49460_row16_col1\" class=\"data row16 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row16_col2\" class=\"data row16 col2\" >0.081320</td>\n",
       "      <td id=\"T_49460_row16_col3\" class=\"data row16 col3\" >0.198913</td>\n",
       "      <td id=\"T_49460_row16_col4\" class=\"data row16 col4\" >-0.874675</td>\n",
       "      <td id=\"T_49460_row16_col5\" class=\"data row16 col5\" >-0.023633</td>\n",
       "      <td id=\"T_49460_row16_col6\" class=\"data row16 col6\" >-0.023697</td>\n",
       "      <td id=\"T_49460_row16_col7\" class=\"data row16 col7\" >-0.034878</td>\n",
       "      <td id=\"T_49460_row16_col8\" class=\"data row16 col8\" >-0.022134</td>\n",
       "      <td id=\"T_49460_row16_col9\" class=\"data row16 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row16_col10\" class=\"data row16 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row16_col11\" class=\"data row16 col11\" >0.000000</td>\n",
       "      <td id=\"T_49460_row16_col12\" class=\"data row16 col12\" >-0.032061</td>\n",
       "      <td id=\"T_49460_row16_col13\" class=\"data row16 col13\" >0.027392</td>\n",
       "      <td id=\"T_49460_row16_col14\" class=\"data row16 col14\" >-0.014563</td>\n",
       "      <td id=\"T_49460_row16_col15\" class=\"data row16 col15\" >-0.143380</td>\n",
       "      <td id=\"T_49460_row16_col16\" class=\"data row16 col16\" >1.000000</td>\n",
       "      <td id=\"T_49460_row16_col17\" class=\"data row16 col17\" >-0.137833</td>\n",
       "      <td id=\"T_49460_row16_col18\" class=\"data row16 col18\" >-0.004319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row17\" class=\"row_heading level0 row17\" >quarter</th>\n",
       "      <td id=\"T_49460_row17_col0\" class=\"data row17 col0\" >0.065949</td>\n",
       "      <td id=\"T_49460_row17_col1\" class=\"data row17 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row17_col2\" class=\"data row17 col2\" >0.018752</td>\n",
       "      <td id=\"T_49460_row17_col3\" class=\"data row17 col3\" >0.024860</td>\n",
       "      <td id=\"T_49460_row17_col4\" class=\"data row17 col4\" >0.007028</td>\n",
       "      <td id=\"T_49460_row17_col5\" class=\"data row17 col5\" >-0.156238</td>\n",
       "      <td id=\"T_49460_row17_col6\" class=\"data row17 col6\" >-0.134650</td>\n",
       "      <td id=\"T_49460_row17_col7\" class=\"data row17 col7\" >-0.098370</td>\n",
       "      <td id=\"T_49460_row17_col8\" class=\"data row17 col8\" >-0.131039</td>\n",
       "      <td id=\"T_49460_row17_col9\" class=\"data row17 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row17_col10\" class=\"data row17 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row17_col11\" class=\"data row17 col11\" >0.000000</td>\n",
       "      <td id=\"T_49460_row17_col12\" class=\"data row17 col12\" >0.034408</td>\n",
       "      <td id=\"T_49460_row17_col13\" class=\"data row17 col13\" >0.131197</td>\n",
       "      <td id=\"T_49460_row17_col14\" class=\"data row17 col14\" >0.004309</td>\n",
       "      <td id=\"T_49460_row17_col15\" class=\"data row17 col15\" >0.970032</td>\n",
       "      <td id=\"T_49460_row17_col16\" class=\"data row17 col16\" >-0.137833</td>\n",
       "      <td id=\"T_49460_row17_col17\" class=\"data row17 col17\" >1.000000</td>\n",
       "      <td id=\"T_49460_row17_col18\" class=\"data row17 col18\" >0.011750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49460_level0_row18\" class=\"row_heading level0 row18\" >after_paycheck</th>\n",
       "      <td id=\"T_49460_row18_col0\" class=\"data row18 col0\" >-0.001084</td>\n",
       "      <td id=\"T_49460_row18_col1\" class=\"data row18 col1\" >0.000000</td>\n",
       "      <td id=\"T_49460_row18_col2\" class=\"data row18 col2\" >0.006860</td>\n",
       "      <td id=\"T_49460_row18_col3\" class=\"data row18 col3\" >0.002311</td>\n",
       "      <td id=\"T_49460_row18_col4\" class=\"data row18 col4\" >-0.002356</td>\n",
       "      <td id=\"T_49460_row18_col5\" class=\"data row18 col5\" >0.078270</td>\n",
       "      <td id=\"T_49460_row18_col6\" class=\"data row18 col6\" >0.097988</td>\n",
       "      <td id=\"T_49460_row18_col7\" class=\"data row18 col7\" >0.061258</td>\n",
       "      <td id=\"T_49460_row18_col8\" class=\"data row18 col8\" >0.079031</td>\n",
       "      <td id=\"T_49460_row18_col9\" class=\"data row18 col9\" >0.000000</td>\n",
       "      <td id=\"T_49460_row18_col10\" class=\"data row18 col10\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row18_col11\" class=\"data row18 col11\" >-0.000000</td>\n",
       "      <td id=\"T_49460_row18_col12\" class=\"data row18 col12\" >0.014570</td>\n",
       "      <td id=\"T_49460_row18_col13\" class=\"data row18 col13\" >-0.080707</td>\n",
       "      <td id=\"T_49460_row18_col14\" class=\"data row18 col14\" >0.070454</td>\n",
       "      <td id=\"T_49460_row18_col15\" class=\"data row18 col15\" >0.009678</td>\n",
       "      <td id=\"T_49460_row18_col16\" class=\"data row18 col16\" >-0.004319</td>\n",
       "      <td id=\"T_49460_row18_col17\" class=\"data row18 col17\" >0.011750</td>\n",
       "      <td id=\"T_49460_row18_col18\" class=\"data row18 col18\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a023fcfd60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr().style.background_gradient(\"YlOrBr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased to 107.78 Mb (72.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Before training model let's optimize it's memory usage\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "  numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "  start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "  for col in df.columns:\n",
    "    col_type = df[col].dtypes\n",
    "\n",
    "    if col_type in numerics:\n",
    "      c_min = df[col].min()\n",
    "      c_max = df[col].max()\n",
    "\n",
    "      if str(col_type)[:3] == \"int\":\n",
    "        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "         df[col] = df[col].astype(np.int8)\n",
    "    \n",
    "        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "         df[col] = df[col].astype(np.int16)\n",
    "    \n",
    "        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "         df[col] = df[col].astype(np.int32)\n",
    "    \n",
    "        elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "         df[col] = df[col].astype(np.int64) \n",
    "\n",
    "      else:\n",
    "        if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "          df[col] = df[col].astype(np.float16)\n",
    "    \n",
    "        elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "          df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "        elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n",
    "          df[col] = df[col].axtype(np.float64)\n",
    " \n",
    "  end_mem = df.memory_usage().sum() / 1024**2\n",
    " \n",
    "  print(\"Memory usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "  return df\n",
    "\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for the model training\n",
    "\n",
    "y = train['sales'].round()\n",
    "X = train.drop(['sales'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3054348,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9709   \u001b[0m | \u001b[0m0.9895   \u001b[0m | \u001b[0m0.2812   \u001b[0m | \u001b[0m0.5985   \u001b[0m | \u001b[0m49.98    \u001b[0m | \u001b[0m24.1     \u001b[0m | \u001b[0m20.17    \u001b[0m | \u001b[0m35.74    \u001b[0m | \u001b[0m74.94    \u001b[0m | \u001b[0m0.4615   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 170\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 170\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 170\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.9165   \u001b[0m | \u001b[0m0.9964   \u001b[0m | \u001b[0m0.7939   \u001b[0m | \u001b[0m0.9862   \u001b[0m | \u001b[0m84.63    \u001b[0m | \u001b[0m12.59    \u001b[0m | \u001b[0m70.77    \u001b[0m | \u001b[0m12.12    \u001b[0m | \u001b[0m67.99    \u001b[0m | \u001b[0m0.258    \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.9798   \u001b[0m | \u001b[95m0.8192   \u001b[0m | \u001b[95m0.8548   \u001b[0m | \u001b[95m0.8278   \u001b[0m | \u001b[95m56.28    \u001b[0m | \u001b[95m26.84    \u001b[0m | \u001b[95m54.7     \u001b[0m | \u001b[95m45.01    \u001b[0m | \u001b[95m62.09    \u001b[0m | \u001b[95m0.4252   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.9602   \u001b[0m | \u001b[0m0.9281   \u001b[0m | \u001b[0m0.5869   \u001b[0m | \u001b[0m0.1144   \u001b[0m | \u001b[0m87.62    \u001b[0m | \u001b[0m23.97    \u001b[0m | \u001b[0m60.78    \u001b[0m | \u001b[0m32.93    \u001b[0m | \u001b[0m25.48    \u001b[0m | \u001b[0m0.8056   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9222   \u001b[0m | \u001b[0m0.9946   \u001b[0m | \u001b[0m0.3264   \u001b[0m | \u001b[0m0.6526   \u001b[0m | \u001b[0m38.59    \u001b[0m | \u001b[0m9.692    \u001b[0m | \u001b[0m45.14    \u001b[0m | \u001b[0m66.6     \u001b[0m | \u001b[0m52.98    \u001b[0m | \u001b[0m0.8559   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 272\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 272\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 272\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m0.8518   \u001b[0m | \u001b[0m0.7594   \u001b[0m | \u001b[0m0.1626   \u001b[0m | \u001b[0m54.26    \u001b[0m | \u001b[0m24.77    \u001b[0m | \u001b[0m51.27    \u001b[0m | \u001b[0m45.05    \u001b[0m | \u001b[0m63.4     \u001b[0m | \u001b[0m0.9023   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.9795   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m73.21    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m54.3     \u001b[0m | \u001b[0m49.52    \u001b[0m | \u001b[0m57.73    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9795   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m63.39    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m74.94    \u001b[0m | \u001b[0m55.51    \u001b[0m | \u001b[0m68.54    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.979    \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m55.23    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m78.04    \u001b[0m | \u001b[0m44.45    \u001b[0m | \u001b[0m43.28    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.338370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9798   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m34.63    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m80.0     \u001b[0m | \u001b[0m33.42    \u001b[0m | \u001b[0m69.56    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9787   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.41    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m80.0     \u001b[0m | \u001b[0m15.95    \u001b[0m | \u001b[0m37.71    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9789   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m53.48    \u001b[0m | \u001b[0m1.949    \u001b[0m | \u001b[0m63.66    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9715   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.4013   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m44.25    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m24.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9591   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.6     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m80.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400366, number of negative: 635866\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1400365, number of negative: 635867\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 2036232, number of used features: 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.01 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789501\n",
      "[LightGBM] [Info] Start training from score 0.789501\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687724 -> initscore=0.789499\n",
      "[LightGBM] [Info] Start training from score 0.789499\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.925    \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m10.08    \u001b[0m | \u001b[0m80.0     \u001b[0m | \u001b[0m2.791    \u001b[0m | \u001b[0m64.81    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6, n_estimators=10000, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "\n",
    "    def lgb_eval(learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf, min_sum_hessian_in_leaf, subsample):\n",
    "        params = {'application': 'binary', 'metric': 'auc'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed,\n",
    "                           stratified=True, verbose_eval=200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "\n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin': (20, 90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf': (0, 100),\n",
    "                                            'subsample': (0.01, 1.0)}, random_state=200)\n",
    "\n",
    "    # n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    # init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "\n",
    "    model_auc = []\n",
    "    for model in range(len(lgbBO.res)):\n",
    "        model_auc.append(lgbBO.res[model]['target'])\n",
    "\n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'], lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n",
    "\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(\n",
    "    X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6, n_estimators=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8192059420307759,\n",
       " 'feature_fraction': 0.854792829599626,\n",
       " 'learning_rate': 0.8277896457253541,\n",
       " 'max_bin': 56,\n",
       " 'max_depth': 27,\n",
       " 'min_data_in_leaf': 55,\n",
       " 'min_sum_hessian_in_leaf': 45.00845042447376,\n",
       " 'num_leaves': 62,\n",
       " 'subsample': 0.4252313651032907,\n",
       " 'objective': 'binary',\n",
       " 'metruc': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out optimized parameters\n",
    "\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1][\"max_depth\"] = int(round(opt_params[1][\"max_depth\"]))\n",
    "opt_params[1][\"min_data_in_leaf\"] = int(round(opt_params[1][\"min_data_in_leaf\"]))\n",
    "opt_params[1][\"max_bin\"] = int(round(opt_params[1][\"max_bin\"]))\n",
    "opt_params[1][\"objective\"] = 'binary'\n",
    "opt_params[1][\"metruc\"] = 'auc'\n",
    "opt_params[1]['is_unbalance'] = True\n",
    "opt_params[1][\"boost_from_average\"] = False\n",
    "opt_params = opt_params[1]\n",
    "opt_params\n",
    "\n",
    "# b_frac: 0.820\n",
    "# f_frac: 0.854\n",
    "# rate: 0.83\n",
    "# max_bin: 56\n",
    "# max_depth 27\n",
    "# min_data_in_leaf: 55\n",
    "# min_sum_hessian_in_leaf: 45\n",
    "# num_leaves: 62\n",
    "# subsample: 0.425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
